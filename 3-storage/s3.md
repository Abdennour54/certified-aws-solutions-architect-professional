# S3

## Storage Classes

- **S3 Standards (default)**:
    - The objects are stored in at least 3 AZs
    - Provides eleven nines of availability
    - The replication is using MD5 file checks together with CRCs to detect object issues
    - When objects are stored in S3 using the API, a HTTP 200 OK response is provided
    - Billing: 
        - GB/month of data stored in S3
        - A dollar for GB charge transfer out (in is free)
        - Price per 1000 requests
        - No specific retrieval fee, no minimum duration, no minimum size
    - S3 standard makes data accessible immediately, can be used for static website hosting
    - Should be used for data frequently accessed
- **S3 Standard-IA**:
    - Shares most of the characteristics of S3 standard: objects are replicated in 3 AZs, durability is the same, availability is the same, first byte latency is the same, objects can be made publicly available
    - Billing:
        - It is more cost effective for storing data
        - Data transfer fee is the same as S3 standard
        - Retrieval fee: for every GB of data there is a retrieval fee, overall cost may increase with frequent data access
        - Minimum duration charge: we will be billed for a minimum of 30 days, minimum capacity of the objects being 128KB (smaller objects will be billed as being 128 KB)
        - Should be used for long lived data where data access is infrequent
- **S3 One Zone-IA**:
    - Similar to S3 standard, but cheaper. Also cheaper that S3 standard IA
    - Data stored using this class is only stored in one region
    - Billing:
        - Similar to S3 standard IA: similar minimum duration fee of 30 days, similar billing for smaller objects and also similar retrieval fee per GB
        - Same level of durability (if the AZ does not fail)
        - Data is replicated inside one AZ
    - Since data is not replicated between AZs, this storage class is not HA. It should be used for non-critical data of for data that can be reproduced easily
- **S3 Glacier**:
    - Same data replication as S3 standard and S3 standard IA
    - Same durability characteristics
    - Storage cost is about 1/5 of S3 standard
    - S3 objects stored in Glacier should be considered cold objects (should not be accessed frequently)
    - Objects in Glacier class are just pointers to real objects and they can not be made public
    - In order to retrieve them, we have to perform a retrieval process:
        - A job that needs to be done to get access to objects
        - Retrievals processes are billed
        - When objects are retrieved for Glacier, they are temporarily stored in standard IA and they are removed after a while. We can retrieve them permanently as well
    - Retrieval process types:
        - **Expedited**: objects are retrieved in 1-5 minutes, retrieval process being the most expensive
        - **Standard**: data is accessible at 3-5 hours
        - **Bulk**: data is accessible at 5-12 hours at lower cost
    - Glacier has a 40KB minimum billable size and a 90 days minimum duration for storage
    - Glacier should be used for data archival, where data can be retrieved in minutes to hours
- **S3 Glacier Deep Archive**:
    - Approximately 1/4 of the price of the Glacier
    - Deep Archive represents data in a frozen state
    - Has a 40KB minimum billable data size and a 180 days minimum duration for data storage
    - Objects can not be made publicly available, data access is similar to standard Glacier class
    - Restore jobs are longer:
        - **Standard**: up to 12 hours
        - **Bulk**: up to 40 hours
    - Should be used for archival which is very rarely accessed
- **S3 Intelligent-Tiering**:
    - It is a storage class containing 4 different tiering a storage
    - Objects that are access frequently are stored in the Frequent Access tier, less frequently accessed objects are stored in the Infrequent Access tier. Objects accessed very infrequently will be stored in either Archive or Deep Archive tier
    -  We don't have to worry for moving objects over tier, this is done by the storage class automatically
    - Intelligent tier can be configured, archiving data is optional and can be enabled/disabled
    - There is no retrieval cost for moving data between frequent and infrequent tiers, we will be billed based on the automation cost per 1000 objects
    - S3 Intelligent-Tiering is recommended for unknown or uncertain data access usage

## S3 Lifecycle Configuration

- We can create lifecycle rules on S3 buckets which can move objects between tiers or expire objects automatically
- A lifecycle configuration is a set of rules applied to a bucket or a group of objects in a bucket
- Rules consist of actions:
    - Transition actions: move objects from one tier to another after a certain time
    - Expiration actions: delete objects or versions of objects
- Objects can not be moved based on how much they are accessed, this can be done by the intelligent tiering. We can move objects based on time passed
- By moving objects from one tier to another we can save costs, expiring objects also will help saving costs
- Transitions between tiers:
![Transition between tiers](images/S3StorageClassesLifecycleConfiguration.png)
- Considerations:
    - Smaller objects cost more in Standard-IA, One Zone-IA, etc.
    - An objects needs to remain for at least 30 days in standard tier before being able to be moved to infrequent tiers (objects can be uploaded manually in infrequent tiers)
    - A single rule can not move objects instantly from standard IA to infrequent tiers and then to Glacier tiers. Objects have to stay for at least 30 days in infrequent tiers before being able to be moved by one rule only. In order ot overcome this, we can define 2 different rules

## S3 Replication

- 2 types of replication are supported by S3:
    - Cross-Region Replication (CRR)
    - Same-Region Replication (SRR)
- Both types of replication support same account replication and cross-account replication
- If we configure cross-account replication, we have to define a policy on the destination account to allow replication from the source account
- We can create replicate all objects from a bucket or we can create rules for a subset of objects
- We can specify which storage class to use for an object in the destination bucket
- We can also define the ownership of the objects in the destination bucket. By default it will be the same as the owner in the source bucket
- Replication Time Control (RTC): if enabled ensures a 15 minutes replication of objects
- Replication consideration:
    - Replication is not retroactive: only newer objects are replicated after the replication is enabled
    - Versioning needs to be enabled for replication
    - Replication is one-way only
    - Replication is capable of handling objects encrypted with SSE-S3 and SSE-KMS. SSE-C is not supported for replication
    - Replication requires for the owner of source bucket needs permissions on the objects which will be replicated
    - System events will not be replicated
    - Any objects in the Glacier and Glacier Deep Archive will not be replicated
    - **Deletion are not replicated!**
- Replication use cases:
    - SRR:
        - Log aggregation
        - PROD and Test sync
        - Resilience with strict sovereignty
    - CRR
        - Global resilience improvements
        - Latency reduction

## S3 Encryption

- Buckets aren't encrypted, objects inside buckets are encrypted
- Encryption at rest types:
    - Client-Side encryption: data is encrypted before it leaves the client
    - Server-Side encryption: data is encrypted at the server side, it is sent on plain-text format from the client
- Both encryption types use encryption in-transit for communication
- There are 3 types of server-side encryption supported:
    - **SSE-C**: server-side encryption with customer-provided keys
        - Customer is responsible for managing the keys, S3 managed encryption
        - When an object is put into S3, we need to provide the key utilized
        - The object will be encrypted by the key, a hash is generated and stored for the key
        - The key will be discarded after the encryption is done
        - In case of object retrieval, we need to provide the key again
    - **SSE-S3**: server-side encryption with Amazon S3-managed keys
        - AWS handles both the encryption/decryption and the key management
        - When using this method, S3 creates a master key for the encryption process (handled entirely by S3)
        - When an object is uploaded an unique key is used for encryption. After the encryption, the with the master key, the unique key is encrypted as well and the unencrypted key is discarded. Both the key and the object are stored
        - For most situations, this is the default type of encryption. It uses AES-256 algorithm, they key management is entirely handled bt S3
    - **SSE-KMS**: Server-side encryption with customer-managed keys stored in AWS Key Management Service (KMS)
        - Similar to SSE-S3, but for this method the KMS handles stored keys
        - When an object is uploaded for the first time, S3 will communicate with KMS an creates a customer master key (CMK). This is default master key used in the future
        - When new objects are uploaded AWS uses the CMK to generate individual keys for encryption (data encryption keys). The data encryption key will be stored along with the object in encrypted format
        - We don't have to use the default CMK provided by AWS, we can use our own CMK. We can control the permission on it and how it is regulated
        - SSE-KMS provides role separation:
            - We can specify who can access the CMK from KMS
            - Administrators can administers buckets but they may not have access to KMS keys
